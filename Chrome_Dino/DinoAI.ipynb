{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import pydirectinput\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Environment\n",
    "\n",
    "class WebGame(Env):\n",
    "    ## setup the environment action and observation shapes\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1,83,100), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "        # Define extraction parameters for the game\n",
    "        self.cap = mss()\n",
    "        self.game_location = {'top': 300, 'left': 0, 'width':600, 'height':500}\n",
    "        self.done_location = {'top': 405, 'left': 630, 'width':660, 'height':70}\n",
    "\n",
    "\n",
    "\n",
    "    ## what is called to do seomthing in the game\n",
    "    def step(self, action):\n",
    "        ## Action key - 0 = space, 1 = Duck(down), 2 = No action (no op)\n",
    "        action_map = {\n",
    "            0: 'space',\n",
    "            1: 'down',\n",
    "            2: 'no_op'\n",
    "        }\n",
    "        if action !=2:\n",
    "            pydirectinput.press(action_map[action])\n",
    "        \n",
    "        ### checking whether the game is done\n",
    "        done, done_cap = self.get_done()\n",
    "        ### Get the next observation\n",
    "        new_observation = self.get_observation()\n",
    "        ## Reward - we get a point for every frame we're alive\n",
    "        reward=1\n",
    "        #Info dictionary\n",
    "        info={}\n",
    "\n",
    "        return new_observation, reward, done, info\n",
    "    \n",
    "\n",
    "\n",
    "    ## Visualize the game\n",
    "    def render(self):\n",
    "        cv2.imshow('Game', np.array(self.cap.grab(self.game_location))[:,:,:3])\n",
    "        # plt.imshow('Game', np.array(self.cap.grab(self.game_location)))\n",
    "        if cv2.waitkey(1) & 0xFF == ord('q'):\n",
    "            self.close\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### Restart the game\n",
    "    def reset(self):\n",
    "        time.sleep(1)\n",
    "        pydirectinput.click(x=150, y=150)\n",
    "        pydirectinput.press('space')\n",
    "        return self.get_observation()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ## This closes down the observation\n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ## Get the part of the observation of the game that we want\n",
    "    def get_observation(self):\n",
    "        ## Get screen capture of the game\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3].astype(np.uint8)\n",
    "        ## Grayscale\n",
    "        gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "        ## Resize\n",
    "        resized = cv2.resize(gray, (100, 83))\n",
    "        ## Add cahnnels first\n",
    "        channel = np.reshape(resized, (1, 83, 100))\n",
    "        return channel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## Get the done text\n",
    "    def get_done(self):\n",
    "        ## Get done screen\n",
    "        done_cap = np.array(self.cap.grab(self.done_location))[:, :, :3]\n",
    "        ## Valid done text\n",
    "        done_strings = ['GAME', 'GAHE']\n",
    "\n",
    "        ## Apply OCR\n",
    "        doen = False\n",
    "        res = pytesseract.image_to_string(done_cap)[:4]\n",
    "        if res in done_strings:\n",
    "            done=True\n",
    "        \n",
    "        return done, done_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ScreenShotError",
     "evalue": "$DISPLAY not set.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mScreenShotError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m env = \u001b[43mWebGame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mWebGame.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mself\u001b[39m.action_space = Discrete(\u001b[32m3\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Define extraction parameters for the game\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28mself\u001b[39m.cap = \u001b[43mmss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mself\u001b[39m.game_location = {\u001b[33m'\u001b[39m\u001b[33mtop\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m300\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m'\u001b[39m:\u001b[32m600\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mheight\u001b[39m\u001b[33m'\u001b[39m:\u001b[32m500\u001b[39m}\n\u001b[32m     12\u001b[39m \u001b[38;5;28mself\u001b[39m.done_location = {\u001b[33m'\u001b[39m\u001b[33mtop\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m405\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m630\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m'\u001b[39m:\u001b[32m660\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mheight\u001b[39m\u001b[33m'\u001b[39m:\u001b[32m70\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/purestorage/AILAB/AI_2/youhans/miniconda3/envs/rl/lib/python3.11/site-packages/mss/factory.py:32\u001b[39m, in \u001b[36mmss\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os_ == \u001b[33m\"\u001b[39m\u001b[33mlinux\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmss\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m linux\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlinux\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMSS\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os_ == \u001b[33m\"\u001b[39m\u001b[33mwindows\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmss\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m windows\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/purestorage/AILAB/AI_2/youhans/miniconda3/envs/rl/lib/python3.11/site-packages/mss/linux.py:286\u001b[39m, in \u001b[36mMSS.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m    285\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33m$DISPLAY not set.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ScreenShotError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(display, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m    289\u001b[39m     display = display.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mScreenShotError\u001b[39m: $DISPLAY not set."
     ]
    }
   ],
   "source": [
    "env = WebGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(None, dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(env.get_observation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 48, 165, 204, ...,  52, 186,  19],\n",
       "        [ 79, 158, 181, ...,   6, 146, 127],\n",
       "        [103, 247,  33, ..., 189, 128, 149],\n",
       "        ...,\n",
       "        [ 57,   3,  53, ...,  50,  64, 237],\n",
       "        [ 67, 102, 218, ..., 195,  33, 188],\n",
       "        [154, 200,  14, ..., 231,  84, 134]]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEst Environment\n",
    "env = WebGame()\n",
    "\n",
    "\n",
    "## Play 10 games\n",
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward=0\n",
    "\n",
    "    while not done:\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "        total_reward += reward\n",
    "    print(f'Total Reward for episode {episode} is {total_reward}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create Callback\n",
    "import os\n",
    "from stable_baseline3.common.callbacks import BaseCallback\n",
    "from stable_baseline3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__()\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './run/checkpoints/'\n",
    "LOG_DIR = './run/logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=1000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build DQN and Train\n",
    "from stable_baseline3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, \n",
    "            buffer_size=1200000, learning_starts=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train model\n",
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model\n",
    "model = DQN.load(os.path.join('train_first', 'best_model_88000'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test model\n",
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(int(action))\n",
    "        # time.sleep(0.01)\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(f'Total Reward for episode {episode} is {total_reward}')\n",
    "    # time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
